# ğŸ›¡ï¸ Towards Trustworthy AI: A Comprehensive Framework for Evaluating Model Safety Across Dimensions

## ğŸ“Œ Overview
This repository provides a **benchmarking framework** for evaluating AI model safety across multiple dimensions. It enables researchers and engineers to assess and compare models on:

- âœ… **Honesty & Transparency** (via TruthfulQA)  
- âœ… **Consistency** (via TruthfulQA)  
- âœ… **Calibration** (via OpenBookQA)  
- âœ… **Bias Mitigation** (via BBQ)  
- âœ… **Deception Resistance** (via TruthfulQA)

By running a suite of standardized tests, youâ€™ll obtain per-dimension scores and an aggregated **Final AI Safety Score** weighted by predefined importance factors.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ .env.sample                    # Template for environment variables (e.g., API keys)
â”œâ”€â”€ .gitignore                     # Files and folders to ignore in Git
â”œâ”€â”€ README.md                      # This documentation
â”œâ”€â”€ main.py                        # CLI entry point (runs benchmarks + analysis)
â”œâ”€â”€ run_all.py                     # Script to run each benchmark and save raw results
â”œâ”€â”€ score.py                       # Aggregates per-dimension scores into final score
â”œâ”€â”€ result_analysis.py             # Generates reliability diagrams & other plots
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ data/                          # Datasets for bias mitigation (BBQ)
â”‚   â””â”€â”€ bbq/
â”‚       â”œâ”€â”€ dataset_dict.json
â”‚       â”œâ”€â”€ dataset_info.json
â”‚       â”œâ”€â”€ state.json
â”‚       â””â”€â”€ test/
â”‚           â””â”€â”€ *.arrow
â”œâ”€â”€ evaluations/                   # One module per safety dimension
â”‚   â”œâ”€â”€ honesty.py
â”‚   â”œâ”€â”€ consistency.py
â”‚   â”œâ”€â”€ calibration.py
â”‚   â”œâ”€â”€ bias_mitigation.py
â”‚   â”œâ”€â”€ deception_resistance.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ results/                       # Outputs generated by benchmark runs
    â”œâ”€â”€ final_report.json          # JSON with detailed per-dimension and final scores
    â””â”€â”€ *reliability_diagram.png   # Calibration plots, etc.
```

---

## ğŸš€ Installation

1. **Clone the repository**  
   ```bash
   git clone https://github.com/your-org/trustworthy-ai-benchmark.git
   cd trustworthy-ai-benchmark
   ```

2. **Set up a virtual environment**  
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**  
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Configure environment variables**  
   Copy the sample and fill in any required keys (e.g. `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`):
   ```bash
   cp .env.sample .env
   # then edit .env with your favorite editor
   ```

---

## â–¶ï¸ Usage

### 1. Run All Benchmarks + Analysis (Recommended)

```bash
python main.py <model_name> [--data_count N]
```

- `<model_name>`: Identifier for your model (e.g. `gpt-4o-mini`).
- `--data_count N`: Optional override of number of examples per test (default: all).

This will:
1. Execute every safety evaluation (honesty, bias, calibration, deception, consistency).  
2. Aggregate scores in `results/final_report.json`.  
3. Produce reliability diagrams and save plots under `results/`.

### 2. Run Benchmarks Only

```bash
python run_all.py --model_name <model_name> --sample_size 10
```

Results are written to `results/final_report.json`.

### 3. Generate Analysis Plots Only

```bash
python result_analysis.py
```

Reads `results/final_report.json` and outputs calibration & reliability diagrams.

---

## ğŸ“Š Interpreting Results

- **`results/final_report.json`**  
  ```json
  {
    "gpt-4o-mini": {
      "honesty": { "honesty_score": 0.85, â€¦ },
      "bias": { "bias_score": 0.92, â€¦ },
      "calibration": { "calibration_score": 0.78, â€¦ },
      "deception": { "resistance_score": 0.81, â€¦ },
      "consistency": { "consistency_score": 0.88, â€¦ },
      "final_score": { "final_safety_score": 0.86 }
    }
  }
  ```
- **Plots**  
  - All the charts of per-dimension performance are generated in `results/` by `result_analysis.py`.

---

## ğŸ¤ Contributing

1. **Fork** the repo and create your feature branch:  
   ```bash
   git checkout -b feature/awesome-test
   ```
2. **Commit** your changes with clear messages.  
3. **Push** to your branch and **open a Pull Request**.  
4. We follow the [Contributor Covenant](https://www.contributor-covenant.org/) â€” please ensure code style consistency and add tests for new benchmarks.

---

## ğŸ“œ License

This project is released under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

Happy benchmarking! ğŸš€  
