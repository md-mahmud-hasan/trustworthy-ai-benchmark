# 🛡️ Towards Trustworthy AI: A Comprehensive Framework for Evaluating Model Safety Across Dimensions

## 📌 Overview
This repository provides a **benchmarking framework** for evaluating AI model safety across multiple dimensions. It enables researchers and engineers to assess and compare models on:

- ✅ **Honesty & Transparency** (via TruthfulQA)  
- ✅ **Consistency** (via TruthfulQA)  
- ✅ **Calibration** (via OpenBookQA)  
- ✅ **Bias Mitigation** (via BBQ)  
- ✅ **Deception Resistance** (via TruthfulQA)

By running a suite of standardized tests, you’ll obtain per-dimension scores and an aggregated **Final AI Safety Score** weighted by predefined importance factors.

---

## 📁 Repository Structure

```
.
├── .env.sample                    # Template for environment variables (e.g., API keys)
├── .gitignore                     # Files and folders to ignore in Git
├── README.md                      # This documentation
├── main.py                        # CLI entry point (runs benchmarks + analysis)
├── run_all.py                     # Script to run each benchmark and save raw results
├── score.py                       # Aggregates per-dimension scores into final score
├── result_analysis.py             # Generates reliability diagrams & other plots
├── requirements.txt               # Python dependencies
├── data/                          # Datasets for bias mitigation (BBQ)
│   └── bbq/
│       ├── dataset_dict.json
│       ├── dataset_info.json
│       ├── state.json
│       └── test/
│           └── *.arrow
├── evaluations/                   # One module per safety dimension
│   ├── honesty.py
│   ├── consistency.py
│   ├── calibration.py
│   ├── bias_mitigation.py
│   ├── deception_resistance.py
│   └── utils.py
└── results/                       # Outputs generated by benchmark runs
    ├── final_report.json          # JSON with detailed per-dimension and final scores
    └── *reliability_diagram.png   # Calibration plots, etc.
```

---

## 🚀 Installation

1. **Clone the repository**  
   ```bash
   git clone https://github.com/your-org/trustworthy-ai-benchmark.git
   cd trustworthy-ai-benchmark
   ```

2. **Set up a virtual environment**  
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**  
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Configure environment variables**  
   Copy the sample and fill in any required keys (e.g. `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`):
   ```bash
   cp .env.sample .env
   # then edit .env with your favorite editor
   ```

---

## ▶️ Usage

### 1. Run All Benchmarks + Analysis (Recommended)

```bash
python main.py <model_name> [--data_count N]
```

- `<model_name>`: Identifier for your model (e.g. `gpt-4o-mini`).
- `--data_count N`: Optional override of number of examples per test (default: all).

This will:
1. Execute every safety evaluation (honesty, bias, calibration, deception, consistency).  
2. Aggregate scores in `results/final_report.json`.  
3. Produce reliability diagrams and save plots under `results/`.

### 2. Run Benchmarks Only

```bash
python run_all.py --model_name <model_name> --sample_size 10
```

Results are written to `results/final_report.json`.

### 3. Generate Analysis Plots Only

```bash
python result_analysis.py
```

Reads `results/final_report.json` and outputs calibration & reliability diagrams.

---

## 📊 Interpreting Results

- **`results/final_report.json`**  
  ```json
  {
    "gpt-4o-mini": {
      "honesty": { "honesty_score": 0.85, … },
      "bias": { "bias_score": 0.92, … },
      "calibration": { "calibration_score": 0.78, … },
      "deception": { "resistance_score": 0.81, … },
      "consistency": { "consistency_score": 0.88, … },
      "final_score": { "final_safety_score": 0.86 }
    }
  }
  ```
- **Plots**  
  - All the charts of per-dimension performance are generated in `results/` by `result_analysis.py`.

---

## 🤝 Contributing

1. **Fork** the repo and create your feature branch:  
   ```bash
   git checkout -b feature/awesome-test
   ```
2. **Commit** your changes with clear messages.  
3. **Push** to your branch and **open a Pull Request**.  
4. We follow the [Contributor Covenant](https://www.contributor-covenant.org/) — please ensure code style consistency and add tests for new benchmarks.

---

## 📜 License

This project is released under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

Happy benchmarking! 🚀  
